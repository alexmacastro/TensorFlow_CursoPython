{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03_Mejorando la Visión por Computador mediante Convoluciones.ipynb","provenance":[{"file_id":"https://github.com/alexmacastro/TensorFlow/blob/master/Course%201%20-%20Part%206%20-%20Lesson%202%20-%20Notebook.ipynb","timestamp":1573739304435}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"R6gHiH-I7uFa"},"source":["# Mejorando la Visión por Computador mediante Convoluciones\n","Autor: [Laurence Moroney](https://www.coursera.org/instructor/lmoroney). TensorFlow in Practice (Coursera--Deep Learning.ai)\n","\n","\\\\\n","Adaptado y traducido por: \\\\\n","\n","Alejandro E. Martínez-Castro amcastro@ugr.es \\\\\n","Departamento de Mecánica de Estructuras e Ingeniería Hidráulica \\\\\n","ETS de Ingeniería de Caminos, Canales y Puertos\n","Universidad de Granada \\\\\n","\n","\n","## Introducción\n","En el cuaderno anterior se mostró una red neuronal profunda (Deep Neuran Network, DNN) para el problema de reconocimiento de prendas de ropa. La red contenía 3 capas: una de entrada, una oculta, y una de salida. \n","\n","Partiremos de este modelo como punto de partida. \n"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1574183094702,"user_tz":-60,"elapsed":29040,"user":{"displayName":"alejandro enrique MARTINEZ CASTRO","photoUrl":"","userId":"00765279006195684870"}},"id":"xcsRtq9OLorS","outputId":"12bcbf56-a5e0-420e-cd7b-5ae24e348f4e","colab":{"base_uri":"https://localhost:8080/","height":512}},"source":["import tensorflow as tf\n","mnist = tf.keras.datasets.fashion_mnist\n","(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n","training_images=training_images / 255.0\n","test_images=test_images / 255.0\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n","  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n","])\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.fit(training_images, training_labels, epochs=5)\n","\n","test_loss = model.evaluate(test_images, test_labels)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","8192/5148 [===============================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","Train on 60000 samples\n","Epoch 1/5\n","60000/60000 [==============================] - 4s 68us/sample - loss: 0.5029 - acc: 0.8245\n","Epoch 2/5\n","60000/60000 [==============================] - 4s 66us/sample - loss: 0.3749 - acc: 0.8660\n","Epoch 3/5\n","60000/60000 [==============================] - 4s 65us/sample - loss: 0.3387 - acc: 0.8758\n","Epoch 4/5\n","60000/60000 [==============================] - 4s 65us/sample - loss: 0.3141 - acc: 0.8843\n","Epoch 5/5\n","60000/60000 [==============================] - 4s 65us/sample - loss: 0.2973 - acc: 0.8906\n","10000/10000 [==============================] - 0s 34us/sample - loss: 0.3572 - acc: 0.8724\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zldEXSsF8Noz"},"source":["La precisión del modelo será probablemente de un 89% en el conjunto de datos de entrenamiento, y un 87% en el de validación. No está mal. Pero, ¿cómo podemos hacerlo mejor?. \n","\n","Una posibilidad es utilizar convoluciones. Las convoluciones son filtros lineales que sirven para acentuar los rasgos, con el fin de mejorar la identificación en el entrenamiento (como se describe en (like this: https://en.wikipedia.org/wiki/Kernel_(image_processing))\n","\n","En resumen, se toma una matriz reducida (de 3x3 o 5x5 usualmente) y se pasa por la imagen. El filtro produce una nueva imagen. Al aplicar el filtro se pierde parte de la resolución inicial: con una ventana de 3x3 sobre una imagen de 28x28, se obtiene una imagen nueva, de 26x26. \n","\n","\n","Los filtros tienen propósito múltiple. Algunos acentúan rasgos horizontales. Otros verticales. Otros simplemente detectan bordes, etc. En en enlace anterior pueden verse varias matrices y filtros (Gaussiano, detector de bordes, etc). \n","\n","Al filtrar una imagen se obtiene una imagen, que contiene los rasgos acentuados. Es más sencilla la identificación, y por tanto, es preferible entrenar sobre la imagen filtrada. \n","\n","Este es el concepto que se maneja en las Redes Neuronales Convolucionales (Convolutional Neural Networks). Se añaden algunas capas para hacer convoluciones justo antes de entrar en las capas densamente conectadas. La información que se pasa a las capas densas están mejor estructuradas, y describen mejor los rasgos. Suelen generar estructuras más precisas. \n","\n","Además de la convolución, se suele realizar otra operación de \"agrupación\" (Pooling). Ésta consiste en, sobre una ventana de 2x2, seleccionar el pixel con el valor más alto, y retenerlo. Tras esta operación, se reduce a la mitad el tamaño procesado. Así, desde una imagen 28x28, que tras una capa convolucional ha quedado con tamaño 26x26 con un filtro de 3x3, tras una agrupación queda en una imagen de 13x13. \n","\n","Normalmente se aplican a la vez operaciones de convolución y agrupamiento, en varias capas. Finalmente, se \"aplana\" la última salida para entrar en una red densa sobre la cual entrenar. \n","\n","Ejecute el bloque de código siguiente. Se trata de la misma red neuronal que se planteó para el caso de prendas de ropa, pero en este caso se añaden capas convolucionales y de pulido. Esta red tardará más en entrenar, pero la precisión mejorará. \n","\n","Para que el modelo corra más rápido, es conveniente elegir un entorno de ejecución donde se emplee la GPU. "]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1574191087476,"user_tz":-60,"elapsed":28946,"user":{"displayName":"alejandro enrique MARTINEZ CASTRO","photoUrl":"","userId":"00765279006195684870"}},"id":"C0tFgT1MMKi6","outputId":"fb68eff3-e839-4c30-aaae-6ddc398b497a","colab":{"base_uri":"https://localhost:8080/","height":667}},"source":["import tensorflow as tf\n","print(tf.__version__)\n","mnist = tf.keras.datasets.fashion_mnist\n","(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n","training_images=training_images.reshape(60000, 28, 28, 1)\n","training_images=training_images / 255.0\n","test_images = test_images.reshape(10000, 28, 28, 1)\n","test_images=test_images/255.0\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n","  tf.keras.layers.MaxPooling2D(2, 2),\n","  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","  tf.keras.layers.MaxPooling2D(2,2),\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dense(10, activation='softmax')\n","])\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.summary()\n","model.fit(training_images, training_labels, epochs=5)\n","test_loss = model.evaluate(test_images, test_labels)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.15.0\n","Model: \"sequential_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_10 (Conv2D)           (None, 26, 26, 64)        640       \n","_________________________________________________________________\n","max_pooling2d_10 (MaxPooling (None, 13, 13, 64)        0         \n","_________________________________________________________________\n","conv2d_11 (Conv2D)           (None, 11, 11, 64)        36928     \n","_________________________________________________________________\n","max_pooling2d_11 (MaxPooling (None, 5, 5, 64)          0         \n","_________________________________________________________________\n","flatten_5 (Flatten)          (None, 1600)              0         \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 128)               204928    \n","_________________________________________________________________\n","dense_11 (Dense)             (None, 10)                1290      \n","=================================================================\n","Total params: 243,786\n","Trainable params: 243,786\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 60000 samples\n","Epoch 1/5\n","60000/60000 [==============================] - 6s 92us/sample - loss: 0.4473 - acc: 0.8380\n","Epoch 2/5\n","60000/60000 [==============================] - 6s 92us/sample - loss: 0.2962 - acc: 0.8923\n","Epoch 3/5\n","60000/60000 [==============================] - 5s 88us/sample - loss: 0.2499 - acc: 0.9085\n","Epoch 4/5\n","60000/60000 [==============================] - 5s 87us/sample - loss: 0.2179 - acc: 0.9192\n","Epoch 5/5\n","60000/60000 [==============================] - 5s 86us/sample - loss: 0.1916 - acc: 0.9282\n","10000/10000 [==============================] - 1s 51us/sample - loss: 0.2856 - acc: 0.8990\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uRLfZ0jt-fQI"},"source":["La nueva red tendrá aproximadamente un 93% en el conjunto de entrenamiento, y un 89.5% en validación. \n","\n","¡ Las convoluciones y agrupamientos han funcionado !\n","\n","Ahora puede probar algo curioso. Pruebe correr el modelo de nuevo (ejecutando de nuevo todo el bloque anterior), pero para más iteraciones (epochs). Por ejemplo, 20. Explore los resultados. \n","\n","Observará que, durante el entrenamiento, la precisión va aumentando. La red detectará cada vez mejor, sobre los datos de entrenamiento, y aparentemente, introducir más iteraciones ha sido beneficioso. \n","\n","Esto no es así. Observe qué ocurre con el conjunto de datos de verificación. La precisión es del 90.89%. No ha mejorado demasiado respecto a la que había con 5 epochs. \n","\n","Lo que observamos es un fenómeno, denominado \"sobre-entrenamiento\" (overfitting). Esto ocurre cuando la red aprende a detectar muy bien sobre los datos de entrenamiento, y en consecuencia, es menos efectiva sobre datos que la red \"no ha visto\" previamente. Es algo así como si toda nuestra vida hubiésemos visto zapatos rojos. Al ver por primera vez un zapato azul, podríamos pensar que no es un zapato, y nos costaría identificarlo.\n","\n","Observe el código de nuevo. Veamos cómo se han construido las convoluciones. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RaLX5cgI_JDb"},"source":["### Construcción del modelo: generación de datos de entrada\n","\n","El paso 1 es recopilar los datos. En este caso se observa que los datos de entrada necesitan ser re-formateados. Esto es así porque la primera convolución necesita partir de un tensor que contenga todo. Por tanto, en vez de 60,000 items de 28x28x1 en una lista, es necesario partir de un tensor de 4 dimensiones, de 60,000x28x28x1. Igual ocurre con las imágenes del conjunto de datos de prueba. Si no se hace esto, se obtendrá un error en el código. \n","\n","Esta es la parte del código que realiza esta función, además de normalizar los valores a [0-1].\n","\n","```\n","import tensorflow as tf\n","mnist = tf.keras.datasets.fashion_mnist\n","(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n","training_images=training_images.reshape(60000, 28, 28, 1)\n","training_images=training_images / 255.0\n","test_images = test_images.reshape(10000, 28, 28, 1)\n","test_images=test_images/255.0\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SS_W_INc_kJQ"},"source":["### Definición del modelo\n","Lo siguiente es definir el modelo. Ahora, en vez de tener la capa de entrada, con neuronas densamente conectadas, en el primer nivel, se van a añadir capas convolucionales previas. \n","\n","Los parámetros son los siguientes: \n","1. El número de convoluciones que se desean generar. Podemos empezar con 64 convoluciones. Esto quiere decir que cada imagen de entrada va a generar 64 variantes. \n","2. El tamaño de la rejilla de convolución: en este caso, 3x3. \n","3. La función de activación a usar. En este caso usaremos `relu`, que devuelve sólo valores positivos, laminando a 0 valores iguales o menores que 0. \n","4. En la primera capa, la forma del dato de entrada.\n","\n","A continuación interviene una capa de tipo `MaxPooling`, que está planteada para comprimir la imagen, manteniendo los rasgos de cada convolución. Especificando (2x2) para MaxPooling, el efecto es reducir la imagen a la cuarta parte (un 25%). \n","\n","Para ver cómo va estructurándose el modelo a través de las sucesivas capas, se recurre a `model.sumary()`, que permite ver el tamaño y la forma de la red. Se observa que tras cada MaxPooling se reduce la imagen. \n","\n","Esta es la parte del código en la que se realiza una convolución y un agrupamiento (Pooling).  \n","\n","```\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n","  tf.keras.layers.MaxPooling2D(2, 2),\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RMorM6daADjA"},"source":["Y otra convolución...\n","\n","\n","\n","```\n","  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","  tf.keras.layers.MaxPooling2D(2,2)\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"b1-x-kZF4_tC"},"source":["\n","Ahora, se \"aplana\" la salida. Es decir, se convierte en un vector. Tras esto, se tendrá una red neuronal del mismo tipo que la que se tenía en la entrada cuando no se usaban convoluciones. El \"aplanado\" se realiza con esta función: \n","\n","```\n","  tf.keras.layers.Flatten(),\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qPtqR23uASjX"},"source":["A continuación se plantean 128 neuronas densamente conectadas, y 10 capas de salida, igual que ocurría en el caso sin usar convoluciones. Lo que se ha hecho con las convoluciones es transformar cada imagen de entrada en muchas imágenes que, cada una de ellas, acentúa un rasgo, y conduce a la misma clasificación que la imagen original. \n","\n","Es como si hubiésemos \"aumentado\" el conjunto de datos de entrada, modificándolo con imágenes más simples, pero mayor en número. \n","\n","Estrictamente, esto no es \"aumentar\" el conjunto de datos de entrada. En TensorFlow existe un procedimiento especial para aumentar ficticiamente los datos de entrada, girando las imágenes, rotándolas, o mostrándolas en espejo, para tener más datos de entrada.\n","\n","\n","\n","```\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dense(10, activation='softmax')\n","])\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"C0GSsjUhAaSj"},"source":["A continuación se compila el modelo, se entrena, y se evalúan las funciones de pérdida y precisión del conjunto de datos de validación. \n","\n","\n","```\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.fit(training_images, training_labels, epochs=5)\n","test_loss, test_acc = model.evaluate(test_images, test_labels)\n","print(test_acc)\n","```\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IXx_LX3SAlFs"},"source":["# Visualización de las Convoluciones y Poolings\n","\n","A continuación visualizaremos las convoluciones gráficamente. La salida de print(test_labels[:100]) nos muestra las primeras 100 etiquetas del conjunto de datos de test. Se observa que los valores en las posiciones 0, 23 y 28 son todas iguales, (valor 9). Esto significa que son botas. \n","\n","Vamos a ver el resultado de correr las convoluciones sobre estas imágenes. Se observarán rasgos comunes. Ahora, cuando la Red Neuronal entrene, tratará de encontrar similitudes entre botas, en base a una combinación de imágenes más sencillas. "]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1574192936792,"user_tz":-60,"elapsed":590,"user":{"displayName":"alejandro enrique MARTINEZ CASTRO","photoUrl":"","userId":"00765279006195684870"}},"id":"f-6nX4QsOku6","outputId":"c1b64038-f4b0-49ed-fe52-a890918152ed","colab":{"base_uri":"https://localhost:8080/","height":73}},"source":["print(test_labels[:100])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n"," 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n"," 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1574193475360,"user_tz":-60,"elapsed":1751,"user":{"displayName":"alejandro enrique MARTINEZ CASTRO","photoUrl":"","userId":"00765279006195684870"}},"id":"9FGsHhv6JvDx","outputId":"8558f5c3-1394-4186-f8a3-7d5d3f803aa0","colab":{"base_uri":"https://localhost:8080/","height":268}},"source":["import matplotlib.pyplot as plt\n","f, axarr = plt.subplots(3,4)\n","FIRST_IMAGE=0\n","SECOND_IMAGE=23\n","THIRD_IMAGE=1\n","CONVOLUTION_NUMBER = 10\n","from tensorflow.keras import models\n","layer_outputs = [layer.output for layer in model.layers]\n","activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n","for x in range(0,4):\n","  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n","  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n","  axarr[0,x].grid(False)\n","  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n","  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n","  axarr[1,x].grid(False)\n","  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n","  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n","  axarr[2,x].grid(False)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAWcAAAD7CAYAAAC2a1UBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 12 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8KVPZqgHo5Ux"},"source":["EJERCICIOS\n","\n","1. Edite las capas de convoluciones. Cambie de 64, a 32 o 16. ¿Qué impacto tiene modificar el número de convoluciones en la precisión y tiempo de entrenamiento?\n","\n","2. Elimine la última capa de convoluciones. ¿Qué ocurre con la precisión y tiempo de entrenamiento?\n","\n","3. ¿Qué ocurre si se añaden más convoluciones?. ¿Qué impacto tiene?. Experimente. \n","\n","4. Elimine todas las convoluciones salvo la primera. ¿Qué sucede?. \n","\n","5. En el cuaderno anterior se implementó una función-objeto callback, para chequear la precisión y parar el entrenamiento cuando se alcance. Pruebe si se puede implementar aquí también."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZpYRidBXpBPM","outputId":"f4f8889e-8cea-48f7-e8bf-a56783fab8c5","executionInfo":{"status":"ok","timestamp":1574194302906,"user_tz":-60,"elapsed":44395,"user":{"displayName":"alejandro enrique MARTINEZ CASTRO","photoUrl":"","userId":"00765279006195684870"}},"colab":{"base_uri":"https://localhost:8080/","height":463}},"source":["import tensorflow as tf\n","print(tf.__version__)\n","mnist = tf.keras.datasets.mnist\n","(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n","training_images=training_images.reshape(60000, 28, 28, 1)\n","training_images=training_images / 255.0\n","test_images = test_images.reshape(10000, 28, 28, 1)\n","test_images=test_images/255.0\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n","  tf.keras.layers.MaxPooling2D(2, 2),\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dense(10, activation='softmax')\n","])\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.fit(training_images, training_labels, epochs=10)\n","test_loss, test_acc = model.evaluate(test_images, test_labels)\n","print(test_acc)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.15.0\n","Train on 60000 samples\n","Epoch 1/10\n","60000/60000 [==============================] - 4s 72us/sample - loss: 0.1468 - acc: 0.9559\n","Epoch 2/10\n","60000/60000 [==============================] - 4s 70us/sample - loss: 0.0503 - acc: 0.9847\n","Epoch 3/10\n","60000/60000 [==============================] - 4s 71us/sample - loss: 0.0311 - acc: 0.9905\n","Epoch 4/10\n","60000/60000 [==============================] - 4s 70us/sample - loss: 0.0205 - acc: 0.9935\n","Epoch 5/10\n","60000/60000 [==============================] - 4s 70us/sample - loss: 0.0149 - acc: 0.9950\n","Epoch 6/10\n","60000/60000 [==============================] - 4s 71us/sample - loss: 0.0102 - acc: 0.9969\n","Epoch 7/10\n","60000/60000 [==============================] - 4s 71us/sample - loss: 0.0075 - acc: 0.9976\n","Epoch 8/10\n","60000/60000 [==============================] - 4s 71us/sample - loss: 0.0067 - acc: 0.9975\n","Epoch 9/10\n","60000/60000 [==============================] - 4s 71us/sample - loss: 0.0054 - acc: 0.9981\n","Epoch 10/10\n","60000/60000 [==============================] - 4s 71us/sample - loss: 0.0045 - acc: 0.9985\n","10000/10000 [==============================] - 1s 51us/sample - loss: 0.0493 - acc: 0.9875\n","0.9875\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H0JFw59xQxK8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}